{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd0c6c2",
   "metadata": {},
   "source": [
    "\n",
    "Created on Mon Sep  5 16:12:50 2022\n",
    "@author: Gianina Meneses Provoste\n",
    "\n",
    "Use: \n",
    "* Estimate velocity per year (or in customizable time periods that overlapp) of continuous GPS time series.\n",
    "\n",
    "Description: \n",
    "\n",
    "- Input: \n",
    "    - zipped data tables containing different signals product of the decomposition of GNSS time series (GrAtSiD, description below)\n",
    "    - id_only.txt (list of station names), ve_EU.txt and vn_EU.txt (?) in a folder declared in variables all_sta_id, fve_eu and vn_eu respectively.  \n",
    "- Output: \n",
    "    - text files in an specific format (see surfacevel2strain input format, Tape et al., 2009) containing geographic coordinates, velocities and errors for the 3 components (east, north, up), initial and final dates and station name.\n",
    "\n",
    "- Velocity can be estimated using 2 approaches: (Df - Di)/t or GrAtSiD modeled velocities. \n",
    "- Surfacevel2strain estimate a velocity field from discrete velocity observations. This approach uses wavelets to interpolate velocity observations, obtaining spatial scale-dependent velocity field, an thus, multiscale velocity derived quantities (strain rate, dilatation rate, rotation, etc).\n",
    "\n",
    "Description input files:\n",
    "- Signals produced by the Greedy Automatic Signal Decomposition (GrAtSiD, Bedford and Bevis, 2018), an algorithm to fit GNSS time series using a multitransient approach.\n",
    "- The zipped tables contain the raw data (in this case time series recording one position per day during 10 years), decomposed signals (artifitial and earthquake steps, nth order polynomial, seasonal, transients, residuals and tectonic velocity among others) and model uncertainty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd2b1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,shutil,glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5dcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA TABLES FOR A PARTICULAR COMPONENT\n",
    "file_e = '../SIGNALS_E.npz'\n",
    "file_n = '../SIGNALS_N.npz'\n",
    "file_u = '../SIGNALS_U.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ff5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses id_only.txt, ve_EU.txt and vn_EU.txt to estimate velocities in a different reference frame (in this case is Europa)\n",
    "all_sta_id = '/home/gmeneses/Documents/RUB/Research/05_Italy_NGL_decomposition_Gianina/vel_files/vel_conversion/EU/id_only.txt'\n",
    "all_ids = []\n",
    "with open(all_sta_id) as ids:\n",
    "    for line in ids:\n",
    "        all_ids.append(line.split()[0])\n",
    "\n",
    "fve_eu = '/home/gmeneses/Documents/RUB/Research/05_Italy_NGL_decomposition_Gianina/vel_files/vel_conversion/EU/ve_EU.txt'\n",
    "fvn_eu = '/home/gmeneses/Documents/RUB/Research/05_Italy_NGL_decomposition_Gianina/vel_files/vel_conversion/EU/vn_EU.txt'\n",
    "ve_eu = []\n",
    "vn_eu = []\n",
    "with open(fve_eu) as fve:\n",
    "    for line in fve:\n",
    "        ve_eu.append(float(line.split()[0]))        \n",
    "with open(fvn_eu) as fvn:\n",
    "    for line in fvn:\n",
    "        vn_eu.append(float(line.split()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bd2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating input data variables\n",
    "##east\n",
    "data_e = np.load(file_e)\n",
    "data_matrix_e = data_e['data_matrix']\n",
    "signals_tensor_e = data_e['signals_tensor']\n",
    "model_uncertainty_tensor_e = data_e['model_uncertainty_tensor'],\n",
    "signals_type_e = data_e['signals_type']\n",
    "# common to all components\n",
    "dates_columns = data_e['dates_columns']\n",
    "coords = data_e['coords']\n",
    "names = data_e['names']\n",
    "\n",
    "### north\n",
    "data_n = np.load(file_n)\n",
    "data_matrix_n = data_n['data_matrix']\n",
    "signals_tensor_n = data_n['signals_tensor']\n",
    "model_uncertainty_tensor_n = data_n['model_uncertainty_tensor'],\n",
    "signals_type_n = data_n['signals_type']\n",
    "\n",
    "## vertical \n",
    "data_u = np.load(file_u)\n",
    "data_matrix_u = data_u['data_matrix']\n",
    "signals_tensor_u = data_u['signals_tensor']\n",
    "model_uncertainty_tensor_u = data_u['model_uncertainty_tensor'],\n",
    "signals_type_u = data_u['signals_type']\n",
    "\n",
    "nsta = data_matrix_e.shape[0]\n",
    "points = data_matrix_e.shape[1]\n",
    "ncompearth = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa05902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing to divide signals by time\n",
    "yini = 2010\n",
    "yfin = 2019\n",
    "#listpoints = np.arange(data_matrix_e.shape[1])\n",
    "#the important here is that each element of list_years corresponds to the first element of the different periods\n",
    "list_years = [x for x in range(yini,yfin + 1)]\n",
    "# list of lists, each element-list contains indexes for specific year\n",
    "# here, if I want to be more general, need to involve more columns in the loop below\n",
    "ix_sep_years = [np.where(dates_columns[:,0] == x)[0] for x in list_years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1151bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each period\n",
    "for i in range(0,len(list_years)):\n",
    "    #A = np.empty((nsta, ncompearth)) \n",
    "    indexes = ix_sep_years[i]\n",
    "    datei = list_years[i] + 0.0\n",
    "    datef = list_years[i] + 0.11\n",
    "    outfile = open('cGPS_GrAtSiD_velocities_IGS14_italy_'+str(datei)+'_'+str(datef)+'_compearth_format.vel','w')\n",
    "    outfile2 = open('cGPS_GrAtSiD_velocities_EU_italy_'+str(datei)+'_'+str(datef)+'_compearth_format.vel','w')\n",
    "    for sta in names:\n",
    "        ix = np.where(names == sta)[0]\n",
    "        ix_allsta = np.where(np.array(all_ids) == sta)[0]\n",
    "        ve_eu = np.array(ve_eu)\n",
    "        vn_eu = np.array(vn_eu)\n",
    "        rest_east = ve_eu[ix_allsta]*1000\n",
    "        rest_north = vn_eu[ix_allsta]*1000\n",
    "        lon = coords[ix,:][0][0]\n",
    "        lat = coords[ix,:][0][1]\n",
    "        #uncertainty for the signal components\n",
    "        unc_e = model_uncertainty_tensor_e[0][ix,:]\n",
    "        unc_n = model_uncertainty_tensor_n[0][ix,:]\n",
    "        unc_u = model_uncertainty_tensor_u[0][ix,:]\n",
    "        #velocities for certain station in mm/y for the whole period, originally in m/d \n",
    "        vel_e = signals_tensor_e[ix,:,7]*1000*365.25\n",
    "        vel_n = signals_tensor_n[ix,:,7]*1000*365.25\n",
    "        vel_u = signals_tensor_u[ix,:,7]*1000*365.25\n",
    "        #velocities station for a certain period\n",
    "        pvel_e = vel_e[0,indexes]\n",
    "        pvel_n = vel_n[0,indexes]\n",
    "        pvel_u = vel_u[0,indexes]\n",
    "        #velocity uncertainties for a certain station and period\n",
    "        puncvel_e = unc_e[0,:,7][indexes]*1000*365.25\n",
    "        puncvel_n = unc_n[0,:,7][indexes]*1000*365.25\n",
    "        puncvel_u = unc_u[0,:,7][indexes]*1000*365.25\n",
    "        #median velocities per period\n",
    "        ve_median = np.nanmedian(pvel_e)\n",
    "        vn_median = np.nanmedian(pvel_n)\n",
    "        vu_median = np.nanmedian(pvel_u)\n",
    "        ve_median_eu = np.nanmedian(pvel_e) - rest_east[0]\n",
    "        vn_median_eu = np.nanmedian(pvel_n) - rest_north[0]\n",
    "        vu_median_eu = vu_median\n",
    "        #median uncertainties per period\n",
    "        rve_median = np.nanmedian(puncvel_e)\n",
    "        rvn_median = np.nanmedian(puncvel_n)\n",
    "        rvu_median = np.nanmedian(puncvel_u)\n",
    "        \n",
    "        if rve_median == 0:\n",
    "            rve_median = 0.01 \n",
    "        if rvn_median == 0:\n",
    "            rvn_median = 0.01\n",
    "        if rvu_median == 0:\n",
    "            rvu_median = 0.01\n",
    "       \n",
    "        string = \" \".join(map(str,[lon, lat, ve_median, vn_median, vu_median, rve_median, rvn_median, rvu_median, 0,0,0, datei, datef, sta]))\n",
    "        print(string,file=outfile)\n",
    "        string_eu = \" \".join(map(str,[lon, lat, ve_median_eu, vn_median_eu, vu_median_eu, rve_median, rvn_median, rvu_median, 0,0,0, datei, datef, sta]))\n",
    "        print(string_eu,file=outfile2)\n",
    "    outfile.close()  \n",
    "    outfile2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f1040a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving velocity files in the directory \"vel_files\"\n",
    "localdir=os.getcwd()\n",
    "os.makedirs(\"./vel_files\",exist_ok=True)\n",
    "\n",
    "for vfile in sorted(glob.glob(localdir+\"/*.vel\")):\n",
    "    name = vfile.split(\"/\")[-1]\n",
    "    shutil.move(localdir+\"/\"+name, \"./vel_files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
